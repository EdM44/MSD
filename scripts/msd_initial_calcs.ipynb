{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import sys, os\n",
    "import numpy as np\n",
    "import fileinput\n",
    "import subprocess\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def smooth(x, window_len=10, window='hanning'):\n",
    "    \"\"\"smooth the data using a window with requested size.\n",
    "    \n",
    "    This method is based on the convolution of a scaled window with the signal. The signal is prepared by introducing reflected copies of the signal \n",
    "    (with the window size) in both ends so that transient parts are minimized in the begining and end part of the output signal.\n",
    "    The Bartlett window is very similar to a triangular window  \n",
    "    input:\n",
    "        x: the input signal \n",
    "        window_len: the dimension of the smoothing window\n",
    "        window: the type of window from 'flat', 'hanning', 'hamming', 'bartlett', 'blackman'\n",
    "            flat window will produce a moving average smoothing.\n",
    "    output:\n",
    "        the smoothed signal\n",
    "    example:\n",
    "    import numpy as np    \n",
    "    t = np.linspace(-2,2,0.1)\n",
    "    x = np.sin(t)+np.random.randn(len(t))*0.1\n",
    "    y = smooth(x)\n",
    "    \n",
    "    see also: \n",
    "    numpy.hanning, numpy.hamming, numpy.bartlett, numpy.blackman, numpy.convolve\n",
    "    scipy.signal.lfilter\n",
    "    \"\"\"\n",
    "    if x.ndim != 1:\n",
    "        raise ValueError, \"smooth only accepts 1 dimension arrays.\"\n",
    "    if x.size < window_len:\n",
    "        raise ValueError, \"Input vector needs to be bigger than window size.\"\n",
    "    if window_len < 3:\n",
    "        return x\n",
    "    if not window in ['flat', 'hanning', 'hamming', 'bartlett', 'blackman']:\n",
    "        raise ValueError, \"Window is on of 'flat', 'hanning', 'hamming', 'bartlett', 'blackman'\"\n",
    "    s=np.r_[2*x[0]-x[window_len:1:-1], x, 2*x[-1]-x[-1:-window_len:-1]]\n",
    "    #print(len(s))\n",
    "    \n",
    "    if window == 'flat': #moving average\n",
    "        w = np.ones(window_len,'d')\n",
    "    else:\n",
    "        w = getattr(np, window)(window_len)\n",
    "    y = np.convolve(w/w.sum(), s, mode='same')\n",
    "    return y[window_len-1:-window_len+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def smoothTriangle(data,degree,dropVals=False):\n",
    "        \"\"\"performs moving triangle smoothing with a variable degree.\"\"\"\n",
    "        \"\"\"note that if dropVals is False, output length will be identical\n",
    "        to input length, but with copies of data at the flanking regions\"\"\"\n",
    "        triangle=numpy.array(range(degree)+[degree]+range(degree)[::-1])+1\n",
    "        smoothed=[]\n",
    "        for i in range(degree,len(data)-degree*2):\n",
    "                point=data[i:i+len(triangle)]*triangle\n",
    "                smoothed.append(sum(point)/sum(triangle))\n",
    "        if dropVals: return smoothed\n",
    "        smoothed=[smoothed[0]]*(degree+degree/2)+smoothed\n",
    "        while len(smoothed)<len(data):smoothed.append(smoothed[-1])\n",
    "        return smoothed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vars = [ 'pr' ]\n",
    "scens = [ 'historical' ]\n",
    "#scens = [ 'historical', 'rcp45', 'rcp85' ]\n",
    "gcms = [ 'obs' ]\n",
    "#gcms =  [ 'gfdl-esm2g','canesm2','cnrm-cm5','csiro-mk3-6-0','inmcm4','miroc5','mpi-esm-lr','mri-cgcm3','ccsm4',\n",
    "#         'noresm1-m','bcc-csm1-1','ipsl-cm5a-lr','ipsl-cm5a-mr','gfdl-cm3','gfdl-esm2m' ]\n",
    "#gcms =  [ 'gfdl-esm2g' ]\n",
    "homedir=os.path.normpath(\"X:/projects/nicaragua/msd\")\n",
    "indir=os.path.normpath(\"X:/zraid_data5/msd/timeseries_sel\")\n",
    "outdir=os.path.normpath(\"X:/projects/nicaragua/msd/timeseries_results_jul_aug\")\n",
    "nyears = 30\n",
    "window_lenth = 31\n",
    "\n",
    "list=\"%s\\%s\" % (homedir,'central_america_lon_lat_elev_selected.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ids=[]\n",
    "lons=[]\n",
    "lats=[]\n",
    "colnames=['Date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    os.stat(outdir)\n",
    "except:\n",
    "    os.mkdir(outdir)\n",
    "\n",
    "for line in fileinput.input([list]):\n",
    "    if not fileinput.isfirstline():\n",
    "        x=line.split(\",\")\n",
    "        ids.append(int(x[0]))\n",
    "        colnames.append(str(x[0]))\n",
    "        lons.append(x[1])\n",
    "        lats.append(x[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#generate column names for file - date followed by one column per cell\n",
    "colnames=range(len(ids))\n",
    "n=['Date']\n",
    "colnames[:0] = n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for scen in scens:\n",
    "    if scen == 'historical':\n",
    "           start_years = [ 1970 ]\n",
    "    else:\n",
    "           start_years = [ 2040, 2070 ]\n",
    "    for v in vars:\n",
    "        for gcm in gcms:\n",
    "            if gcm == 'obs':\n",
    "                fname = indir + \"/\" + gcm+ \"_\" + v + \"_ts.txt\"\n",
    "            else:\n",
    "                fname = indir + \"/\" + gcm+ \"_\" + scen + \"_\" + v + \"_ts.txt\"\n",
    "            fnamegz = fname + \".gz\"\n",
    "            if os.path.isfile(fnamegz):\n",
    "                print 'Unzipping' + fnamegz\n",
    "                proc=subprocess.Popen(['gzip', '-d',fnamegz])\n",
    "                proc.wait()\n",
    "            print 'Reading ' + fname \n",
    "            x=pd.read_csv(fname,delim_whitespace=True,header=None,names=colnames,index_col='Date',infer_datetime_format=True,parse_dates='Date')\n",
    "            print 'Finished reading, zipping infile'\n",
    "            p1=subprocess.Popen(['gzip',fname])\n",
    "            p1.wait()\n",
    "\n",
    "            for start_yr in start_years:\n",
    "                ey=start_yr+nyears-1\n",
    "                print \"processing period \" + str(start_yr) + \"-\" + str(ey)\n",
    "                #make empty data frame with one row per grid cell-results for years will be appended as columns\n",
    "                onset_max=pd.DataFrame(index=range(x.shape[1]))\n",
    "                onset_day=pd.DataFrame(index=range(x.shape[1]))\n",
    "                end_max=pd.DataFrame(index=range(x.shape[1]))\n",
    "                end_day=pd.DataFrame(index=range(x.shape[1]))\n",
    "                val_min=pd.DataFrame(index=range(x.shape[1]))\n",
    "                duration=pd.DataFrame(index=range(x.shape[1]))\n",
    "                avg_max=pd.DataFrame(index=range(x.shape[1]))\n",
    "                intensity=pd.DataFrame(index=range(x.shape[1]))\n",
    "                msd_stats=pd.DataFrame(index=range(x.shape[1]))\n",
    "                msd_stats.index_name='N'\n",
    "\n",
    "                for yr in range(start_yr,ey+1):\n",
    "                    subx = x[datetime(yr, 5, 1):datetime(yr,9, 30)]\n",
    "                    subxplus = x[datetime(yr, 4, 1):datetime(yr,10, 31)]\n",
    "                    if subx[0].count() < 30:\n",
    "                        continue\n",
    "                    #subx_smooth=subx.apply(smooth,args=(window_len=window_lenth,window='bartlett'))\n",
    "                    subx_smooth=pd.DataFrame(index=subx.index)\n",
    "                    #can't figure out how to use apply with this...\n",
    "                    for col in range(subx.shape[1]):\n",
    "                        dat_sm=smooth(np.array(subxplus[col]),window_len=window_lenth,window='bartlett')\n",
    "                        dat_smplus=pd.DataFrame(dat_sm,index=subxplus.index)\n",
    "                        dat_smooth=dat_smplus[datetime(yr, 5, 1):datetime(yr,9, 30)]\n",
    "                        subx_smooth = pd.concat([subx_smooth,dat_smooth],axis=1,join_axes=[subx.index])\n",
    "                    subx_smooth.columns = subx.columns\n",
    "                    #subx_smooth now has ndays rows x ncells columns for the current year, indexed by date\n",
    "                    #Calculate statistics for current year - all columns at once\n",
    "                    onset_max1yr=subx_smooth[0:int(subx.shape[0]/2)].apply(np.max)\n",
    "                    onset_date1yr=subx_smooth[0:int(subx.shape[0]/2)].apply(lambda x: x.idxmax())\n",
    "                    end_max1yr=subx_smooth[int(subx.shape[0]/2):int(subx.shape[0])].apply(np.max)\n",
    "                    end_date1yr=subx_smooth[int(subx.shape[0]/2):int(subx.shape[0])].apply(lambda x: x.idxmax())\n",
    "                    duration1yr=end_date1yr.dt.dayofyear.sub(onset_date1yr.dt.dayofyear)\n",
    "                    #duration1yr=pd.DataFrame(end_date1yr.dt.dayofyear.values-onset_date1yr.dt.dayofyear.values)\n",
    "                    #if(end_date-onset_date)...how to find minimum between two dates at each cell?\n",
    "                    junk=[]\n",
    "                    for col in range(subx_smooth.shape[1]):\n",
    "                        junk.append(np.min(subx_smooth[col][onset_date1yr[col]:end_date1yr[col]]))\n",
    "                    val_min1yr=pd.DataFrame(np.array(junk),index=onset_max.index)\n",
    "                    avg_max1yr=end_max1yr.add(onset_max1yr)\n",
    "                    avg_max1yr=avg_max1yr/2.0\n",
    "                    #intensity1yr=avg_max1yr.sub(val_min1yr)\n",
    "                    #intensity1yr=pd.DataFrame(((end_max1yr.values+onset_max1yr.values)/2.0)-val_min1yr.values)\n",
    "                \n",
    "                    #add current year's data to output dataframe for period\n",
    "                    avg_max=pd.concat([avg_max,avg_max1yr],axis=1,join_axes=[avg_max.index])\n",
    "                    onset_max=pd.concat([onset_max,onset_max1yr],axis=1,join_axes=[onset_max.index])\n",
    "                    onset_day=pd.concat([onset_day,onset_date1yr.dt.dayofyear],axis=1,join_axes=[onset_day.index])\n",
    "                    end_max=pd.concat([end_max,end_max1yr],axis=1,join_axes=[end_max.index])\n",
    "                    end_day=pd.concat([end_day,end_date1yr.dt.dayofyear],axis=1,join_axes=[end_day.index])\n",
    "                    val_min=pd.concat([val_min,val_min1yr],axis=1,join_axes=[val_min.index])\n",
    "                    duration=pd.concat([duration,duration1yr],axis=1,join_axes=[duration.index])\n",
    "                    #intensity=pd.concat([intensity,intensity1yr],axis=1,join_axes=[intensity.index])\n",
    "\n",
    "                intensity=avg_max.sub(val_min)\n",
    "                #print output statistics\n",
    "                msd_stats['ID']=np.array(ids, dtype=int)\n",
    "                msd_stats['Lon']=np.array(lons)\n",
    "                msd_stats['Lat']=np.array(lats)\n",
    "                msd_stats['nyears']=onset_max.count(axis=1)\n",
    "                msd_stats['onset_mx_mn']=onset_max.mean(axis=1)\n",
    "                msd_stats['onset_dy_mn']=onset_day.mean(axis=1)\n",
    "                msd_stats['end_mx_mn']=end_max.mean(axis=1)\n",
    "                msd_stats['end_dy_mn']=end_day.mean(axis=1)\n",
    "                msd_stats['min_mn_mn']=val_min.mean(axis=1)\n",
    "                msd_stats['duration_mn']=duration.mean(axis=1)\n",
    "                msd_stats['intensity_mn']=intensity.mean(axis=1)\n",
    "                msd_stats['onset_mx_sd']=onset_max.std(axis=1)\n",
    "                msd_stats['onset_dy_sd']=onset_day.std(axis=1)\n",
    "                msd_stats['end_mx_sd']=end_max.std(axis=1)\n",
    "                msd_stats['end_dy_sd']=end_day.std(axis=1)\n",
    "                msd_stats['min_mn_sd']=val_min.std(axis=1)\n",
    "                msd_stats['duration_sd']=duration.std(axis=1)\n",
    "                msd_stats['intensity_sd']=intensity.std(axis=1)\n",
    "                foutname = outdir + \"/\" + gcm + \"_\" + scen + \"_\"+v+\"_\"+str(start_yr)+\"-\"+str(ey)+\"_stats.csv\"\n",
    "                print \"writing output to file \" + foutname\n",
    "                try:\n",
    "                    os.remove(foutname)\n",
    "                except OSError:\n",
    "                    pass\n",
    "                msd_stats.to_csv(foutname, index=False, float_format='%.4f')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
